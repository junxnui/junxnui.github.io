<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-122759872-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

     gtag('config', 'UA-122759872-1');
    </script>
    
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<link rel="shortcut icon" HREF="index/jx.ico">
    <title>Jun Xing</title>
    <link rel="stylesheet" type="text/css" href="./index/main.css">
    <link href="./index/css" rel="stylesheet">
    <style>
        .quote{
            font-family: 'Dawning of a New Day';
            font-weight:bold;
            font-size:30px;
        }
    </style>
    
</head>
<body>
    <div id="main">
        
        <div class="name">
            Jun Xing 邢骏    
        </div>
        
        <div>
            <a href="https://www.bilibili.com/video/BV1GV411i7wR?p=1"><img src="./index/lumi.PNG" style="width:200px;" class="portrait"></a><br>
            &nbsp;&nbsp;&nbsp;&nbsp;
            
        
            <div class="bio_format">
                <p>I am <b>NOT</b> working at <a href="http://www.mihayo.com/">miHoYo (米哈游)</a> now. Previously, I was a postdoc researcher at the <a href ="http://ict.usc.edu/">Vision and Graphics group of Institute for Creative  Technologies</a> working with Dr. <a href = "http://www.hao-li.com/">Hao Li</a>. Prior to that, I got my PhD in CS from the <a href="http://www.cs.hku.hk">University of Hong Kong</a>, under the supervision of Dr. <a href="http://www.liyiwei.org">Li-Yi Wei</a>, and my B.S. from <a href="http://www.ustc.edu.cn">University of Science and Technology of China (USTC)</a>.</p>
                
                <p>My interest focuses on building the high-end lifelike digital human, with research covering performance capturing and retargeting, motion synthesis and stylization, voice generation, reinforcement learning, chatbot, etc.. Contact me if you are interested to working with me.
                </p>
                
                <!---<a href="index/cv.pdf">CV</a>&nbsp;--->
            <a href="https://scholar.google.com/citations?hl=zh-TW&user=qT4hAYwAAAAJ&view_op=list_works" target="blank"><img src="index/scholar_logo.png" width="25px"></a>&nbsp;
            <A href="mailto:junxnui@gmail.com"><img src="index/email_icon.png" width="25px"></A>&nbsp;
            <a href="https://hk.linkedin.com/in/xing-jun-90737071" target="blank"><img src="index/linkedin_logo.png" width="25px"></a>
                
                <!---<a href="https://www.facebook.com/jun.xing.1293" target="blank"><img src="index/facebook_logo.png" width="25px"></a>&nbsp;--->
                
            </div>
        </div>
                
        
        <!---
        <div>
          <span class="category">News</span>
          <hr class="line">
			<ul class="item_content">
                <li>
                    We are hiring researchers and interns for digital human research at miHoYo, <A href="mailto:jun.xing@mihoyo.com"> contact me </A> if you are interested.
                </li>
                <li>
                    [03/2021] I will serve as deputy director of the Joint Laboratory between Ruijin Hospital Brain Disease Center and miHoYo.
                </li>
                <li>
                    [03/2021] One paper accepted to CVPR 2021.
                </li>
                <li>
                    [02/2020] Two papers accepted to CVPR 2020.
                </li>
                <li>
                    [08/2019] I will serve on the International Programe Committee of AAAI 2020, IJCAI 2020.
                </li>
                <li>
                    [07/2019] One paper accepted to SIGGRAPH Asia 2019.
                </li>
                <li>
                    [07/2019] One paper accepted to ICCV 2019 Oral.
                </li>
                <li>
                    [06/2019] One paper accepted to UIST 2019.
                </li>
                <li>
                    [04/2019] One submission accepted to SIGGRAPH 2019 Real-Time Live!
                </li>
                <li>
                    [02/2019] One paper accepted to CVPR 2019 and one paper accepted to IEEE VR 2019.
                </li>
                <li>
                    [01/2019] Started my new job at miHoYo in Shanghai, China.
                </li>
                <li>
                    [12/2018] paGAN is demoed in SIGGRAPH Asia 2018 Real-Time Live!
                </li>
                <li>
                    [08/2018] I will serve on the International Programe Committee of AAAI 2019 and CVM 2019.
                </li>
                <li>
                    [08/2018] paGAN is featured in fxguide, LA Times, CBC, CBS, and Netflix/Buzzfeed.
                </li>
                <li>
                    [08/2018] One paper accepted to SIGGRAPH Asia 2018.
                </li>
                <li>
                    [07/2018] Two papers accepted to ECCV 2018 and one paper accepted to BMVC 2018.
                </li>
            </ul>
        </div>--->
        
        
        <div>
          <span class="category">Professional</span>
          <hr class="line">
			<ol class="item_content" style="padding:0px;list-style-type:none">
            <!---<li>
                    Joint Laboratory between Ruijin Hospital Brain Disease Center and miHoYo, Deputy Director, Shanghai, March. 2021 - ongoing
            </li>--->
            <li>
                    <a href="http://www.mihayo.com/" target="blank"><img src="index/mihoyo_logo.png" height="15px"> miHoYo (米哈游)</a>, Lead Researcher, Shanghai, Jan. 2019 - Jan. 2024
            </li>
            <li>
                    <a href="http://ict.usc.edu/" target="blank"><img src="index/USC_logo.jpg" height="15px"> University of Southern California</a>, Postdoc in the Vision and Graphics group, Los Angeles, May 2017 - Jan. 2019
            </li>
            <li>
                <a href="http://www.adobe.com/technology.html" target="blank"><img src="index/adobe_logo.png" height="15px"> Adobe</a>, Procedural Imaging Group Intern, San Jose, July 2016 - Sep. 2016
            </li>
            <li>
                <a href="https://www.autodeskresearch.com/" target="blank"><img src="index/autodesk_logo.png" height="15px"> Autodesk Research</a>, UI Graphics Research Intern, Toronto, Jan. 2016 - April 2016
            </li>
            <li>
                <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="blank"><img src="index/microsoft_logo.png" height="15px"> Microsoft Research Asia</a>, Graphics Research Intern, Beijing, Dec. 2014 - April 2015
            </li>
            </ol>
        </div>
        
        
        <div>
			<span class="category">Publications</span>
            <hr class="line">
			<ol style="padding:0px;list-style-type:none">
                
                <li class="item_format" style="position:relative">
                    <img src="./research/kd.png" class="img_format">
                    <div class="title_format">
                        <b>Revisiting Knowledge Distillation: An Inheritance and Exploration Framework </b>
                        <span class="author_format">Zhen Huang, Xu Shen, <b>Jun Xing</b>, Tongliang Liu, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, Xian-Sheng Hua</span>
                        <span class="venue_format"><em>CVPR 2021</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We propose a novel inheritance and exploration knowledge distillation framework. The inheritance part is learned with a similarity loss to transfer the existing learned knowledge from the teacher model to the student model, while the exploration part is encouraged to learn representations different from the inherited ones with a dis-similarity loss.">abstract</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                	
                <li class="item_format" style="position:relative">
                    <img src="./research/facialHairEdit.png" class="img_format">
                    <div class="title_format">
                        <b>Intuitive, Interactive Beard and Hair Synthesis with Generative Models </b>
                        <span class="author_format">Kyle Olszewski, Duygu Ceylan, <b>Jun Xing</b>, Jose I. Echevarria, Zhili Chen, Weikai Chen, Hao Li</span>
                        <span class="venue_format"><em>CVPR 2020 (Oral Presentation)</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We present an interactive approach to synthesizing realistic variations in facial hair in images, ranging from subtle edits to existing hair to the addition of complex and challenging hair in images of clean-shaven subjects.">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2004.06848.pdf">paper</a>]
                            [<a href="https://www.youtube.com/watch?v=v4qOtBATrvM">youtube</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/faceModel.png" class="img_format">
                    <div class="title_format">
                        <b>Learning Formation of Physically-based Face Attributes </b>
                        <span class="author_format">Ruilong Li, Kalle Bladin, Yajie Zhao, Chinmay Chinara, Owen Ingraham, Pengda Xiang, Xinglei Ren, Pratusha Prasad, Biping Kishore, <b>Jun Xing</b>, Hao Li</span>
                        <span class="venue_format"><em>CVPR 2020</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We introduce a non-linear morphable face model, capable of producing multifarious face geometry of pore-level resolution, coupled with material attributes for use in physically-based rendering environments.">abstract</a>]
                            [<a href="https://vgl.ict.usc.edu/Research/Deep3DMM/04724.pdf">paper</a>]
                            [<a href="https://vgl.ict.usc.edu/Research/Deep3DMM/">project</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/faceNorm.png" class="img_format">
                    <div class="title_format">
                        <b>Deep Face Normalization</b>
                        <span class="author_format">Koki Nagano, Huiwen Luo, Zejian Wang, Jaewoo Seo, <b>Jun Xing</b>, Liwen Hu, Lingyu Wei, Hao Li</span>
                        <span class="venue_format"><em>SIGGRAPH Asia 2019</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We present a deep learning framework that can fully normalize unconstrained face images, i.e., remove perspective distortions, relight to an evenly lit environment, and predict a frontal and neutral face.">abstract</a>]
                            [<a href="http://luminohope.org/SIG2019_Portrait_Normalization.pdf">paper</a>]
                            [<a href="http://luminohope.org/SIG2019_Portrait_Normalization_supp.pdf">supp</a>]
                            [<a href="http://youtube.com/watch?v=fzA9SBJlUDY">youtube</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/undistortion.png" class="img_format">
                    <div class="title_format">
                        <b>Learning Perspective Undistortion of Portraits</b>
                        <span class="author_format">Yajie Zhao, Zeng Huang, Tianye Li, Weikai Chen, Chloe LeGendre, Xinglei Ren, <b>Jun Xing</b>, Ari Shapiro, Hao Li</span>
                        <span class="venue_format"><em>ICCV 2019 (Oral Presentation) </em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We present a deep learning based approach to remove perspective distortion from unconstrained portraits.">abstract</a>]
                            [<a href="https://arxiv.org/pdf/1905.07515.pdf">paper</a>]
                            [<a href="https://github.com/bearjoy730/Learning-Perspective-Undistortion-of-Portraits">project</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/uist19.gif" class="img_format">
                    <div class="title_format">
                        <b>HairBrush for Immersive Data-Driven Hair Modeling</b>
                        <span class="author_format"><b>Jun Xing</b>, Koki Nagano, Weikai Chen, Haotian Xu, Li-Yi Wei, Yajie Zhao, Jingwan Lu, Byungmoon Kim, Hao Li</span>
                        <span class="venue_format"><em>UIST 2019</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We propose an interactive hair modeling system that can help create complex hairstyles that would otherwise take days or weeeks with existing tools.">abstract</a>]
                            [<a href="http://www.hao-li.com/publications/papers/uist2019HBIDDHM.pdf">paper</a>]
                            [<a href="https://www.youtube.com/watch?v=_1jchaP1M3E">youtube</a>/<a href="http://www.hao-li.com/publications/movies/uist2019video.mp4">video</a>]
                            [<a href="research/hairVR.zip">code</a>]
                        </div>
                        <div class="link_format">
                            medias: [<a href="https://beforesandafters.com/2019/07/16/its-like-youve-just-stepped-out-of-a-vr-hair-salon/?fbclid=IwAR0pspHDwVAfwzh0gDGeSUR30-yjhsX9X6r2NrdBUCeGED3MMt-klWyl8DY">Befores&Afters</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/hairvr.png" class="img_format">
                    <div class="title_format">
                        <b>VR Hair Salon for Avatars</b>
                        <span class="author_format"><b>Jun Xing</b>, Liwen Hu, Koki Nagano, Li-Yi Wei, Hao Li</span>
                        <span class="venue_format"><em>SIGGRAPH 2019 Real-Time Live!</em></span>
                        <div class="link_format">
                             [<a class="btn btn-default abstract" abstract="We demo a VR system that allows modelers, even novices, to create rich hairstyles including buns, braids, strands, eyebrows, and beards within minutes/hours that would take weeks/months with other tools. Our system autocompletes sparse user gestures in real-time via a deep neural network trained from an artist-curated hair data set.">abstract</a>]
                            [<a href="https://s2019.siggraph.org/conference/programs-events/real-time-live/">info</a>]
                            [<a href = "https://dl.acm.org/citation.cfm?id=3333568">doc</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/cvpr19_qnet.png" class="img_format">
                    <div class="title_format">
                        <b>Quantization Networks</b>
                        <span class="author_format">Jiwei Yang, Xu Shen, <b>Jun Xing</b>, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, Xiansheng Hua</span>
                        <span class="venue_format"><em>CVPR 2019</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We provides a simple/straightforward and general/uniform solution for any-bit weights and activations quantization.">abstract</a>]
                            [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Quantization_Networks_CVPR_2019_paper.pdf">paper</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/maskoff.png" class="img_format">
                    <div class="title_format">
                        <b>Mask-off: Synthesizing Face Images in the Presence of Head-mounted Displays</b>
                        <span class="author_format">Yajie Zhao, Qingguo Xu, Weikai Chen, <b>Jun Xing</b>, Du Chao, Xinyu Huang, Ruigang Yang</span>
                        <span class="venue_format"><em>IEEE VR 2019</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We proposes a new system for synthesizing facial images with head-mounted displays on.">abstract</a>]
                            [coming soon]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/dytexture_mike.jpg" class="img_format">
                    <div class="title_format">
                        <b>paGAN: Real-time Avatars Using Dynamic Textures</b>
                        <span class="author_format">Koki Nagano, Jaewoo Seo, <b>Jun Xing</b>, Lingyu Wei, Zimo Li, Shunsuke Saito, Aviral Agarwal, Jens Fursund, Hao Li</span>
                        <span class="venue_format"><em>SIGGRAPH Asia 2018</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="A deep learning-based technology for generating photo-realistic 3D avatars with dynamic facial textures from a single input image is presented. Real-time performance-driven animations and renderings are demonstrated on an iPhone X.">abstract</a>]
                            [<a href="http://www.hao-li.com/publications/papers/siggraphAsia2018PAGAN.pdf">paper</a>]
                            [<a href="https://www.youtube.com/watch?v=_JMx5VN1eeM&feature=youtu.be">youtube</a>/<a href="http://www.hao-li.com/publications/movies/siggraphAsia2018videoA.mp4">video</a>/<a href="https://www.youtube.com/watch?v=wdKpXvF_3AU">trailer</a>]
                        </div>
                        <div class="link_format">
                            medias: [<a href="https://www.fxguide.com/featured/a-i-at-siggraph-part-2-pinscreen-at-real-time-live/">fxguide</a>]
                            [<a href="http://www.latimes.com/business/technology/la-fi-tn-fake-videos-20180219-story.html">LA Times</a>]
                            [<a href="https://www.cbsnews.com/news/spotting-fake-news-in-a-world-with-manipulated-video/">CBS</a>]
                            [<a href="https://www.cbc.ca/news/fifth/the-deepfake-the-war-over-truth-the-lie-detectors-1.4910865">CBC</a>]
                            [<a href="http://www.news24.jp/articles/2018/11/02/10408304.html?fbclid=IwAR3Y6YAvcWXEqrMNEncm-FuIYyrJ5dFhuSOQ0aU-k3xN-gs-LAYtTcGDTUU">NTV</a>]
                            [<a href="https://www.cartoonbrew.com/tech/12-cool-new-pieces-of-animation-tech-we-saw-at-siggraph-2018-163072.html">Cartoon brew</a>]
                            [<a href="https://www.netflix.com/title/80217889">Netflix/Buzzfeed</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/siga19_RTL.jpg" class="img_format">
                    <div class="title_format">
                        <b>Pinscreen Avatars in your Pocket: Mobile paGAN engine and Personalized Gaming</b>
                        <span class="author_format">Koki Nagano, Shunsuke Saito, Mclean Goldwhite, Kyle San, Aaron Hong, Liwen Hu, Lingyu Wei, <b>Jun Xing</b>, Qingguo Xu, Hanwei Kung, Jiale Kuang, Aviral Agarwal, Erik Castellanos, Jaewoo Seo, Jens Fursund, Hao Li</span>
                        <span class="venue_format"><em>SIGGRAPH Asia 2018 Real-Time Live!</em></span>
                        <div class="link_format">
                             [<a class="btn btn-default abstract" abstract="We will demonstrate how a lifelike 3D avatar can be instantly built from a single selfie input image using our own team members as well as a volunteer from the audience. We will showcase some additional 3D avatars built from internet photographs, and highlight the underlying technology such as our light-weight real-time facial tracking system. Then we will show how our automated rigging system enables facial performance capture as well as full body integration. We will showcase different body customization features and other digital assets, and show various immersive applications such as 3D selfie themes, multi-player games, all running on an iPhone.">abstract</a>]
                            [<a href = "https://dl.acm.org/citation.cfm?id=3289162">doc</a>]
                            [<a href="https://sa2018.siggraph.org/en/attendees/real-time-live">info</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/dytexture_chris.jpg" class="img_format">
                    <div class="title_format">
                        <b>Deep Learning-Based Photoreal Avatars for Online Virtual Worlds in iOS</b>
                        <span class="author_format">Koki Nagano, Jaewoo Seo, Kyle San, Aaron Hong, Mclean Goldwhite, 
                            <b>Jun Xing</b>, Jiale Kuang, Aviral Agarwal, Caleb Arthur, Hanwei Kung, Stuti Rastogi, Carrie Sun,
                            Stephen Chen, Jens Fursund, Hao Li</span>
                        <span class="venue_format"><em>SIGGRAPH 2018 Real-Time Live!</em></span>
                        <div class="link_format">
                             [<a class="btn btn-default abstract" abstract="A deep learning-based technology for generating photo-realistic 3D avatars with dynamic facial textures from a single input image is presented. Real-time performance-driven animations and renderings are demonstrated on an iPhone X and we show how these avatars can be integrated into compelling virtual worlds and used for 3D chats.">abstract</a>]
                            [<a href="https://dl.acm.org/citation.cfm?id=3229237">doc</a>]
                            [<a href="https://www.youtube.com/watch?v=KgnD7tdlwUo&feature=youtu.be">Live demo</a>]
                            [<a href="https://www.youtube.com/watch?v=UDIP6lndP5g&feature=youtu.be">youtube</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/singleviewhair.png" class="img_format">
                    <div class="title_format">
                        <b>HairNet: Single-View Hair Reconstruction using Convolutional Neural Networks</b>
                        <span class="author_format">Yi Zhou, Liwen Hu, <b>Jun Xing</b>, Weikai Chen, Han-Wei Kung, Xin Tong, Hao Li</span>
                        <span class="venue_format"><em>ECCV 2018</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We introduce a deep learning-based method to generate full 3D hair geometry from an unconstrained image. Our method can recover local strand details and has real-time performance.">abstract</a>]
                            [<a href="https://arxiv.org/abs/1806.07467">paper</a>]
                            [<a href="https://www.youtube.com/watch?v=MLnS-gTWc9w">youtube</a>]
                        </div>
                        <div class="link_format">
                            medias: [<a href="https://news.developer.nvidia.com/ai-can-render-hair-in-3d-in-real-time/">Nvidia News</a>]
                            [<a href="https://www.technologyreview.com/s/611569/the-best-of-the-physics-arxiv-week-ending-june-30-2018/">MIT Tech Review</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/multiviewbody.png" class="img_format">
                    <div class="title_format">
                        <b>Deep Volumetric Video From Very Sparse Multi-View Performance Capture</b>
                        <span class="author_format">Zeng Huang, Tianye Li, Weikai Chen, Yajie Zhao,
                              <b>Jun Xing</b>, Chloe LeGendre, Linjie Luo, Chongyang Ma, Hao Li</span>
                        <span class="venue_format"><em>ECCV 2018</em></span>
                        <div class="link_format">
                             [<a class="btn btn-default abstract" abstract="We present a fully-automatic lightweight system that is capable of capturing human performance from highly sparse, e.g. three, views, with the subject wearing general apparel.">abstract</a>]
                            [<a href="research/eccv18body.pdf">paper</a>] 
                            [<a href="https://www.youtube.com/watch?v=sQnXlQ3GyKc&feature=youtu.be">youtube</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/inpainting.png" class="img_format">
                    <div class="title_format">
                        <b>Identity Preserving Face Completion for Large Ocular Region Occlusion</b>
                        <span class="author_format">Yajie Zhao, Weikai Chen, <b>Jun Xing</b>, Xiaoming Li, Zach Bessinger,
                            Fuchang Liu, Wangmeng Zuo, Ruigang Yang</span>
                        <span class="venue_format"><em>BMVC 2018</em></span>
                        <div class="link_format">
                             [<a class="btn btn-default abstract" abstract="We present a novel deep learning approach to synthesize full face images in the presence of large ocular region occlusions while keeping the identities.">abstract</a>]
                            [<a href="https://arxiv.org/abs/1807.08772">paper</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/yoda.png" class="img_format">
                    <div class="title_format">
                        <b>Autocomplete 3D Sculpting</b>
                        <span class="author_format">Mengqi Peng, <b>Jun Xing</b>, Li-Yi Wei</span>
                        <span class="venue_format"><em>SIGGRAPH 2018</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We propose an interactive 3D sculpting system that assists users in freely creating models without predefined scope.">abstract</a>]
                            [<a href="research/sig18_autocomplete_3D_sculpting.pdf">paper</a>] 
                            [<a href="https://www.youtube.com/watch?v=wcQy42XWTnA&feature=youtu.be">youtube</a>]
                            [<a href="https://1iyiwei.github.io/a3s-sig18/">project</a>]
                        </div>
                        <div class="link_format">
                            medias: [<a href="https://www.technologyreview.com/s/604113/the-best-of-the-physics-arxiv-week-ending-april-8-2017/">MIT Tech Review</a>]
                            [<a href="http://3dnchu.com/archives/autocomplete-3d-sculpting/">3dnchu</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/highfreq.png" class="img_format">
                    <div class="title_format">
                        <b>Mesoscopic Facial Geometry Inference using Deep Neural Networks</b>
                        <span class="author_format">Loc Huynh, Weikai Chen, Shunsuke Saito, <b>Jun Xing</b>, Koki Nagano, Andrew Jones, Hao Li, Paul Debevec</span>
                        <span class="venue_format"><em>CVPR 2018 (Spotlight Presentation)</em><br></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We present a learning-based approach for synthesizing facial geometry at medium and fine scales from diffusely-lit facial texture maps.">abstract</a>]
                            [<a href="research/cvpr2018_MFGIDNN.pdf">paper</a>] 
                            [<a href="https://www.youtube.com/watch?v=Rg8iYS7TG48&feature=youtu.be">youtube</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/slr.png" class="img_format">
                    <div class="title_format">
                        <b>Sequence-to-Sequence Learning via Shared Latent Representation</b>
                        <span class="author_format">Xu Shen, Xinmei Tian, <b>Jun Xing</b>, Yong Rui, Dacheng Tao</span>
                        <span class="venue_format"><em>AAAI 2018</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We propose a star-like framework for general and flexible sequence-to-sequence learning, where different types of media contents (the peripheral nodes) could be encoded to and decoded from a shared latent representation (the central node).">abstract</a>]
                            [<a href="research/aaai18_srl.pdf">paper</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/energyBrush.gif" class="img_format">
                    <div class="title_format">
                        <b>Energy Brushes: Interactive Tools for Illustrating Stylized Elemental Dynamics</b>
                        <span class="author_format"><b>Jun Xing</b>, Rubaiat Habib Kazi, Tovi Grossman, Li-Yi Wei, Jos Stam and George Fitzmaurice</span>
                        <span class="venue_format"><em>UIST 2016</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We present a new animation framework and interactive system that enables artists to design elemental dynamics by sketching the underlying forces.">abstract</a>]
                            [<a href="research/uist2016_energybrush_animated.pdf">paper</a>] 
                            [<a href="https://www.youtube.com/watch?v=qj2XxB2dsco">youtube</a>]
                            [<a href="https://www.autodeskresearch.com/publications/energy-brushes">project</a>]
                            [<a href="https://youtu.be/URl46eU-JNM?list=PLqhXYFYmZ-VcUPus2QYpAZdFmw5w6seVR">presentation</a>/<a href="research/EnergyBrushUIST.pptx">slides</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/tom.gif" class="img_format">
                    <div class="title_format">
                        <b>Autocomplete Hand-drawn Animations</b>
                        <span class="author_format"><b>Jun Xing</b>, Li-Yi Wei, Takaaki Shiratori and Koji Yatani</span>
                        <span class="venue_format"><em>SIGGRAPH Asia 2015</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We present an interactive drawing system that helps users produce animation more easily and in a better quality while preserving manual drawing practices.">abstract</a>]
                            [<a href="research/siga15_autocomplete_handdrawn_animations.pdf">paper</a>] 
                            [<a href="https://www.youtube.com/watch?v=w0YmWiy6sA4">youtube</a>] 
                            [<a href="http://www.liyiwei.org/papers/workflow-siga15/">project</a>]
                        </div>
                        <div class="link_format">
                            medias: [<a href="http://www.wired.com/2015/10/microsofts-badass-new-tool-is-like-autocomplete-for-drawing/">Wired</a>] 
                            [<a href="http://www.fastcodesign.com/3052463/microsoft-research-debuts-autocomplete-for-animation-and-its-incredible">fastCompany</a>]
                            [<a href="https://www.techtimes.com/articles/98210/20151021/microsofts-new-tool-autocomplete-animations.htm">techTimes</a>]
                            [<a href="http://3dnchu.com/archives/autocomplete-hand-drawn-anim/">3dnchu</a>]
                            [<a href="http://mentalfloss.com/article/70202/autocomplete-software-now-exists-hand-drawn-animation">mentalFloss</a>]
                            [<a href="https://www.coolthings.com/autocomplete-animation-microsoft-research/">coolThings</a>]
                            [<a href="https://cgpress.org/archives/autocomplete-hand-drawn-animations.html">CGPress</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/willow.png" class="img_format">
                    <div class="title_format">
                        <b>Autocomplete Painting Repetitions</b>
                        <span class="author_format"><b>Jun Xing</b>, Hsiang-Ting Chen and Li-Yi Wei</span>
                        <span class="venue_format"><em>SIGGRAPH Asia 2014</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We present an interactive digital painting system that autocompletes tedious repetitions while preserving nuanced variations and maintaining natural flows.">abstract</a>]
                            [<a href="research/siga14_autocomplete_painting_repetitions.pdf">paper</a>] 
                            [<a href="https://www.youtube.com/watch?v=m7MEAw46Ojo">youtube</a>] 
                            [<a href="http://www.liyiwei.org/papers/workflow-siga14/">project</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
	        </ol>
        </div>


        <!---
        <div>
            <span class="category">Exhibitions</span>
            <hr class="line">
            <ol style="padding:0px;list-style-type:none">
                <li class="item_format" style="position:relative">
                    <img src="./research/hairvr.png" class="img_format">
                    <div class="title_format">
                        <b>VR Hair Salon for Avatars</b>
                        <span class="author_format"><b>Jun Xing</b>, Liwen Hu, Koki Nagano, Li-Yi Wei, Hao Li</span>
                        <span class="venue_format"><em>SIGGRAPH 2019 Real-Time Live!</em></span>
                        <div class="link_format">
                             [<a class="btn btn-default abstract" abstract="We demo a VR system that allows modelers, even novices, to create rich hairstyles including buns, braids, strands, eyebrows, and beards within minutes/hours that would take weeks/months with other tools. Our system autocompletes sparse user gestures in real-time via a deep neural network trained from an artist-curated hair data set.">abstract</a>]
                            [<a href="https://s2019.siggraph.org/conference/programs-events/real-time-live/">info</a>]
                            [<a href = "https://dl.acm.org/citation.cfm?id=3333568">doc</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/siga19_RTL.jpg" class="img_format">
                    <div class="title_format">
                        <b>Pinscreen Avatars in your Pocket: Mobile paGAN engine and Personalized Gaming</b>
                        <span class="author_format">Koki Nagano, Shunsuke Saito, Mclean Goldwhite, Kyle San, Aaron Hong, Liwen Hu, Lingyu Wei, <b>Jun Xing</b>, Qingguo Xu, Hanwei Kung, Jiale Kuang, Aviral Agarwal, Erik Castellanos, Jaewoo Seo, Jens Fursund, Hao Li</span>
                        <span class="venue_format"><em>SIGGRAPH Asia 2018 Real-Time Live!</em></span>
                        <div class="link_format">
                             [<a class="btn btn-default abstract" abstract="We will demonstrate how a lifelike 3D avatar can be instantly built from a single selfie input image using our own team members as well as a volunteer from the audience. We will showcase some additional 3D avatars built from internet photographs, and highlight the underlying technology such as our light-weight real-time facial tracking system. Then we will show how our automated rigging system enables facial performance capture as well as full body integration. We will showcase different body customization features and other digital assets, and show various immersive applications such as 3D selfie themes, multi-player games, all running on an iPhone.">abstract</a>]
                            [<a href = "https://dl.acm.org/citation.cfm?id=3289162">doc</a>]
                            [<a href="https://sa2018.siggraph.org/en/attendees/real-time-live">info</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/dytexture_chris.jpg" class="img_format">
                    <div class="title_format">
                        <b>Deep Learning-Based Photoreal Avatars for Online Virtual Worlds in iOS</b>
                        <span class="author_format">Koki Nagano, Jaewoo Seo, Kyle San, Aaron Hong, Mclean Goldwhite, 
                            <b>Jun Xing</b>, Jiale Kuang, Aviral Agarwal, Caleb Arthur, Hanwei Kung, Stuti Rastogi, Carrie Sun,
                            Stephen Chen, Jens Fursund, Hao Li</span>
                        <span class="venue_format"><em>SIGGRAPH 2018 Real-Time Live!</em></span>
                        <div class="link_format">
                             [<a class="btn btn-default abstract" abstract="A deep learning-based technology for generating photo-realistic 3D avatars with dynamic facial textures from a single input image is presented. Real-time performance-driven animations and renderings are demonstrated on an iPhone X and we show how these avatars can be integrated into compelling virtual worlds and used for 3D chats.">abstract</a>]
                            [<a href="https://dl.acm.org/citation.cfm?id=3229237">doc</a>]
                            [<a href="https://www.youtube.com/watch?v=KgnD7tdlwUo&feature=youtu.be">Live demo</a>]
                            [<a href="https://www.youtube.com/watch?v=UDIP6lndP5g&feature=youtu.be">youtube</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>

            </ol>
        </div>
        --->
        

        <div>
            <span class="category">PhD Thesis</span>
            <hr class="line">
            <ol style="padding:0px;list-style-type:none">
                <li class="item_format" style="position:relative">
                    <img src="./index/hku_logo.jpg" class="img_format">
                    <div class="title_format">
                        <b>Autocomplete Hand-drawn Sketches and Animations</b> <br>
                        <span class="author_format">Jun Xing &nbsp; &nbsp; &nbsp; &nbsp; University of Hong Kong</span>
                        [<a href="index/thesis.pdf">thesis</a>] &nbsp; &nbsp;
                        This thesis is basically a concatenation of my first three projects.
                    </div>
                    <div style="clear:both;"></div>
                </li>

            </ol>
        </div>


</div>

    <script src="./index/canvas-nest.js_1.0.1_canvas-nest.min.js" opacity="0.6" color="0,68,255" zindex="-1"></script><canvas id="c_n1" width="1287" height="736" style="position: fixed; top: 0px; left: 0px; z-index: -1; opacity: 0.6;"></canvas>




</body></html>
