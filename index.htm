<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-122759872-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

     gtag('config', 'UA-122759872-1');
    </script>
    
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<link rel="shortcut icon" HREF="index/jx.ico">
    <title>Jun Xing</title>
    <link rel="stylesheet" type="text/css" href="./index/main.css">
    <link href="./index/css" rel="stylesheet">
    <style>
        .quote{
            font-family: 'Dawning of a New Day';
            font-weight:bold;
            font-size:30px;
        }
    </style>
    
</head>
<body>
    <div id="main">
        
        <div class="name">
            Jun Xing 邢骏    
        </div>
        
        <div>
            <img src="./index/jun.jpg" style="width:200px;" class="portrait"><br>
              &nbsp;&nbsp;&nbsp;
        
            <div class="bio_format">
                <p> I am now a postdoc at the <a href ="http://ict.usc.edu/">Vision and Graphics group of Institute for Creative  Technologies</a> working with Dr. <a href = "http://www.hao-li.com/">Hao Li</a>. Prior to that, I got my PhD in CS from the <a href="http://www.cs.hku.hk">University of Hong Kong</a>, under the supervision of Dr. <a href="http://www.liyiwei.org">Li-Yi Wei</a>, and my B.S. from <a href="http://www.ustc.edu.cn">University of Science and Technology of China (USTC)</a>.</p>
                
                <p>My research combines modern concepts in computer graphics, computer vision, machine learning and human computer interaction, with broad applications in 2D/3D digital contents analysis, synthesis and authoring. In particular, I am interested in <em>interactive/predictive</em> modeling and <em>deep learning-based</em> reconstruction of high-fidelity face, hair and body for digital human.
                </p>
                
                Here is my <a href="index/cv.pdf">CV</a>&nbsp;
                <A href="mailto:junxnui@gmail.com"><img src="index/email_icon.png" width="25px"></A>&nbsp;
                <a href="https://hk.linkedin.com/in/xing-jun-90737071" target="blank"><img src="index/linkedin_logo.png" width="25px"></a>&nbsp;
                <a href="https://www.youtube.com/c/JunXing" target="blank"><img src="index/youtube_logo.png" width="25px"></a>&nbsp;
                <a href="https://plus.google.com/+JunXing" target="blank"><img src="index/googleplus_logo.png" width="25px"></a>&nbsp;
                <a href="https://www.facebook.com/jun.xing.1293" target="blank"><img src="index/facebook_logo.png" width="25px"></a>&nbsp;
            </div>
        </div>
                
        
        <div>
          <span class="category">Recent News</span>
          <hr class="line">
			<ul class="item_content">
                <li>
                    [08/2018] I will serve on the International Programe Committee of AAAI 2019 and CVM 2019.
                </li>
                <li>
                    [08/2018] One paper conditionally accepted to SIGGRAPH Asia 2018.
                </li>
                <li>
                    [07/2018] Two papers accepted to ECCV 2018.
                </li>
                <li>
                    [07/2018] One paper accepted to BMVC 2018. 
                </li>
                <li>
                    [05/2018] One submission accepted to SIGGRAPH 2018 Real-Time Live!
                </li>
                <li>
                    [04/2018] I will serve on the International Program Committee of Pacific Graphics 2018.
                </li>
                <li>
                    [04/2018] One paper accepted to SIGGRAPH 2018.
                </li>
                <li>
                    [03/2018] One paper accepted to CVPR 2018 for Spotlight presentation.
                </li>
            </ul>
        </div>
        
        
        <div>
          <span class="category">Work Experience</span>
          <hr class="line">
			<ol class="item_content" style="padding:0px;list-style-type:none">
            <li>
                    <a href="http://ict.usc.edu/" target="blank"><img src="index/USC_logo.jpg" height="15px"> University of Southern California</a>, Postdoc in the Vision and Graphics group, Los Angeles, May 1, 2017 - Present
            </li>
            <li>
                <a href="http://www.adobe.com/technology.html" target="blank"><img src="index/adobe_logo.png" height="15px"> Adobe</a>, Procedural Imaging Group Intern, San Jose, July 11, 2016 - Sep. 30, 2016
            </li>
            <li>
                <a href="https://www.autodeskresearch.com/" target="blank"><img src="index/autodesk_logo.png" height="15px"> Autodesk Research</a>, UI Graphics Research Intern, Toronto, Jan. 11, 2016 - April 30, 2016
            </li>
            <li>
                <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="blank"><img src="index/microsoft_logo.png" height="15px"> Microsoft Research Asia</a>, Graphics Research Intern, Beijing, Dec. 08, 2014 - April 30, 2015
            </li>
            </ol>
        </div>
        
        
        <div>
			<span class="category">Publications</span>
            <hr class="line">
			<ol style="padding:0px;list-style-type:none">
                	
                <li class="item_format" style="position:relative">
                    <img src="./research/dytexture.jpg" class="img_format">
                    <div class="title_format">
                        <b>paGAN: Real-time Avatars Using Dynamic Textures</b>
                        <span class="author_format">Koki Nagano, Jaewoo Seo, Lingyu Wei, <b>Jun Xing</b>, Shunsuke Saito, Zimo Li, Aviral Agarwal, Jens Fursund, Hao Li</span>
                        <span class="venue_format"><em>SIGGRAPH Asia 2018</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="A deep learning-based technology for generating photo-realistic 3D avatars with dynamic facial textures from a single input image is presented. Real-time performance-driven animations and renderings are demonstrated on an iPhone X and we show how these avatars can be integrated into compelling virtual worlds and used for 3D chats.">abstract</a>]
                            [<a href="https://www.fxguide.com/featured/a-i-at-siggraph-part-2-pinscreen-at-real-time-live/">fxguide</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/singleviewhair.png" class="img_format">
                    <div class="title_format">
                        <b>HairNet: Single-View Hair Reconstruction using Convolutional Neural Networks</b>
                        <span class="author_format">Yi Zhou, Liwen Hu, <b>Jun Xing</b>, Weikai Chen, Han-Wei Kung, Xin Tong, Hao Li</span>
                        <span class="venue_format"><em>ECCV 2018</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We introduce a deep learning-based method to generate full 3D hair geometry from an unconstrained image. Our method can recover local strand details and has real-time performance.">abstract</a>]
                            [<a href="https://arxiv.org/abs/1806.07467">paper</a>]
                            [<a href="https://www.youtube.com/watch?v=MLnS-gTWc9w">youtube</a>]
                            [<a href="https://news.developer.nvidia.com/ai-can-render-hair-in-3d-in-real-time/">Nvidia News</a>]
                            [<a href="https://www.technologyreview.com/s/611569/the-best-of-the-physics-arxiv-week-ending-june-30-2018/">MIT Tech Review</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/multiviewbody.png" class="img_format">
                    <div class="title_format">
                        <b>Deep Volumetric Video From Very Sparse Multi-View Performance Capture</b>
                        <span class="author_format">Zeng Huang, Tianye Li, Weikai Chen, Yajie Zhao,
                              <b>Jun Xing</b>, Chloe LeGendre, Linjie Luo, Chongyang Ma, Hao Li</span>
                        <span class="venue_format"><em>ECCV 2018</em></span>
                        <div class="link_format">
                             [<a class="btn btn-default abstract" abstract="We present a fully-automatic lightweight system that is capable of capturing human performance from highly sparse, e.g. three, views, with the subject wearing general apparel.">abstract</a>]
                            [<a href="research/eccv18body.pdf">paper</a>] 
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/inpainting.png" class="img_format">
                    <div class="title_format">
                        <b>Identity Preserving Face Completion for Large Ocular Region Occlusion</b>
                        <span class="author_format">Yajie Zhao, Weikai Chen, <b>Jun Xing</b>, Xiaoming Li, Zach Bessinger,
                            Fuchang Liu, Wangmeng Zuo, Ruigang Yang</span>
                        <span class="venue_format"><em>BMVC 2018</em></span>
                        <div class="link_format">
                             [<a class="btn btn-default abstract" abstract="We present a novel deep learning approach to synthesize full face images in the presence of large ocular region occlusions while keeping the identities.">abstract</a>]
                            [<a href="https://arxiv.org/abs/1807.08772">paper</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/yoda.png" class="img_format">
                    <div class="title_format">
                        <b>Autocomplete 3D Sculpting</b>
                        <span class="author_format">Mengqi Peng, <b>Jun Xing</b>, Li-Yi Wei</span>
                        <span class="venue_format"><em>SIGGRAPH 2018</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We propose an interactive 3D sculpting system that assists users in freely creating models without predefined scope.">abstract</a>]
                            [<a href="research/sig18_autocomplete_3D_sculpting.pdf">paper</a>] 
                            [<a href="https://www.youtube.com/watch?v=wcQy42XWTnA&feature=youtu.be">youtube</a>]
                            [<a href="https://1iyiwei.github.io/a3s-sig18/">project</a>]
                            [<a href="https://www.technologyreview.com/s/604113/the-best-of-the-physics-arxiv-week-ending-april-8-2017/">MIT Tech Review</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/highfreq.png" class="img_format">
                    <div class="title_format">
                        <b>Mesoscopic Facial Geometry Inference using Deep Neural Networks</b>
                        <span class="author_format">Loc Huynh, Weikai Chen, Shunsuke Saito, <b>Jun Xing</b>, Koki Nagano, Andrew Jones, Hao Li, Paul Debevec</span>
                        <span class="venue_format"><em>CVPR 2018 (Spotlight Presentation)</em><br></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We present a learning-based approach for synthesizing facial geometry at medium and fine scales from diffusely-lit facial texture maps.">abstract</a>]
                            [<a href="research/cvpr2018_MFGIDNN.pdf">paper</a>] 
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/slr.png" class="img_format">
                    <div class="title_format">
                        <b>Sequence-to-Sequence Learning via Shared Latent Representation</b>
                        <span class="author_format">Xu Shen, Xinmei Tian, <b>Jun Xing</b>, Yong Rui, Dacheng Tao</span>
                        <span class="venue_format"><em>AAAI 2018</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We propose a star-like framework for general and flexible sequence-to-sequence learning, where different types of media contents (the peripheral nodes) could be encoded to and decoded from a shared latent representation (the central node).">abstract</a>]
                            [<a href="research/aaai18_srl.pdf">paper</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                
                <li class="item_format" style="position:relative">
                    <img src="./research/energyBrush.gif" class="img_format">
                    <div class="title_format">
                        <b>Energy Brushes: Interactive Tools for Illustrating Stylized Elemental Dynamics</b>
                        <span class="author_format"><b>Jun Xing</b>, Rubaiat Habib Kazi, Tovi Grossman, Li-Yi Wei, Jos Stam and George Fitzmaurice</span>
                        <span class="venue_format"><em>UIST 2016</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We present a new animation framework and interactive system that enables artists to design elemental dynamics by sketching the underlying forces.">abstract</a>]
                            [<a href="research/uist2016_energybrush_animated.pdf">paper</a>] 
                            [<a href="https://www.youtube.com/watch?v=qj2XxB2dsco">youtube</a>]
                            [<a href="https://www.autodeskresearch.com/publications/energy-brushes">project</a>]
                            [<a href="https://youtu.be/URl46eU-JNM?list=PLqhXYFYmZ-VcUPus2QYpAZdFmw5w6seVR">presentation</a>/<a href="research/EnergyBrushUIST.pptx">slides</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/tom.gif" class="img_format">
                    <div class="title_format">
                        <b>Autocomplete Hand-drawn Animations</b>
                        <span class="author_format"><b>Jun Xing</b>, Li-Yi Wei, Takaaki Shiratori and Koji Yatani</span>
                        <span class="venue_format"><em>SIGGRAPH Asia 2015</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We present an interactive drawing system that helps users produce animation more easily and in a better quality while preserving manual drawing practices.">abstract</a>]
                            [<a href="research/siga15_autocomplete_handdrawn_animations.pdf">paper</a>] 
                            [<a href="https://www.youtube.com/watch?v=w0YmWiy6sA4">youtube</a>] 
                            [<a href="http://www.liyiwei.org/papers/workflow-siga15/">project</a>]
                            [<a href="http://www.wired.com/2015/10/microsofts-badass-new-tool-is-like-autocomplete-for-drawing/">Wired</a>] 
                            [<a href="http://www.fastcodesign.com/3052463/microsoft-research-debuts-autocomplete-for-animation-and-its-incredible">Fastcompany</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <img src="./research/willow.png" class="img_format">
                    <div class="title_format">
                        <b>Autocomplete Painting Repetitions</b>
                        <span class="author_format"><b>Jun Xing</b>, Hsiang-Ting Chen and Li-Yi Wei</span>
                        <span class="venue_format"><em>SIGGRAPH Asia 2014</em></span>
                        <div class="link_format">
                            [<a class="btn btn-default abstract" abstract="We present an interactive digital painting system that autocompletes tedious repetitions while preserving nuanced variations and maintaining natural flows.">abstract</a>]
                            [<a href="research/siga14_autocomplete_painting_repetitions.pdf">paper</a>] 
                            [<a href="https://www.youtube.com/watch?v=m7MEAw46Ojo">youtube</a>] 
                            [<a href="http://www.liyiwei.org/papers/workflow-siga14/">project</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>
                
	        </ol>
        </div>


        <div>
            <span class="category">Exhibitions</span>
            <hr class="line">
            <ol style="padding:0px;list-style-type:none">
                <li class="item_format" style="position:relative">
                    <img src="./research/rtl_mike.png" class="img_format">
                    <div class="title_format">
                        <b>Deep Learning-Based Photoreal Avatars for Online Virtual Worlds in iOS</b>
                        <span class="author_format">Koki Nagano, Jaewoo Seo, <b>Jun Xing</b>, Kyle San, Aaron Hong, Mclean Goldwhite,
                            Jiale Kuang, Aviral Agarwal, Caleb Arthur, Hanwei Kung, Stuti Rastogi, Carrie Sun,
                            Stephen Chen, Jens Fursund, Hao Li</span>
                        <span class="venue_format"><em>SIGGRAPH 2018 Real-time Live!</em></span>
                        <div class="link_format">
                             [<a class="btn btn-default abstract" abstract="A deep learning-based technology for generating photo-realistic 3D avatars with dynamic facial textures from a single input image is presented. Real-time performance-driven animations and renderings are demonstrated on an iPhone X and we show how these avatars can be integrated into compelling virtual worlds and used for 3D chats.">abstract</a>]
                            [<a href="https://www.youtube.com/watch?v=KgnD7tdlwUo&feature=youtu.be">Live demo</a>]
                            [<a href="https://www.youtube.com/watch?v=UDIP6lndP5g&feature=youtu.be">youtube</a>]
                        </div>
                    </div>
                    <div style="clear:both;"></div>
                </li>

            </ol>
        </div>
        

        <div>
            <span class="category">PhD Thesis</span>
            <hr class="line">
            <ol style="padding:0px;list-style-type:none">
                <li class="item_format" style="position:relative">
                    <img src="./index/hku_logo.jpg" class="img_format">
                    <div class="title_format">
                        <b>Autocomplete Hand-drawn Sketches and Animations</b> <br>
                        <span class="author_format">Jun Xing &nbsp; &nbsp; &nbsp; &nbsp; University of Hong Kong</span>
                        [<a href="index/thesis.pdf">thesis</a>] &nbsp; &nbsp;
                        This thesis is basically a concatenation of my first three projects.
                    </div>
                    <div style="clear:both;"></div>
                </li>

            </ol>
        </div>


</div>

    <script src="./index/canvas-nest.js_1.0.1_canvas-nest.min.js" opacity="0.6" color="0,68,255" zindex="-1"></script><canvas id="c_n1" width="1287" height="736" style="position: fixed; top: 0px; left: 0px; z-index: -1; opacity: 0.6;"></canvas>




</body></html>